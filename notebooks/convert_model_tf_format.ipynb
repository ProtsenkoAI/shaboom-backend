{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b912bcc-1ede-46f1-91b5-8520ac4e965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pipenv install tflite-support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22475f9f-1189-4b5b-85c8-369cab1963fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "playsound is relying on another python subprocess. Please use `pip install pygobject` if you want playsound to run more efficiently.\n",
      "2021-09-22 18:01:13.780462: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-09-22 18:01:13.780512: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "import IPython.display as ipd\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import demucs\n",
    "import demucs.utils\n",
    "import demucs.separate\n",
    "import demucs.pretrained\n",
    "\n",
    "from resampy import resample\n",
    "import playsound\n",
    "import sounddevice as sd\n",
    "\n",
    "from tensorflow.keras.layers import Input, Reshape, Conv2D, BatchNormalization\n",
    "from tensorflow.keras.layers import MaxPool2D, Dropout, Permute, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12dec1d9-46bd-4535-98b5-28b3d34d2cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_load_model(model_capacity, filename):\n",
    "    \"\"\"\n",
    "    Build the CNN model and load the weights\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_capacity : 'tiny', 'small', 'medium', 'large', or 'full'\n",
    "        String specifying the model capacity, which determines the model's\n",
    "        capacity multiplier to 4 (tiny), 8 (small), 16 (medium), 24 (large),\n",
    "        or 32 (full). 'full' uses the model size specified in the paper,\n",
    "        and the others use a reduced number of filters in each convolutional\n",
    "        layer, resulting in a smaller model that is faster to evaluate at the\n",
    "        cost of slightly reduced pitch estimation accuracy.\n",
    "    Returns\n",
    "    -------\n",
    "    model : tensorflow.keras.models.Model\n",
    "        The pre-trained keras model loaded in memory\n",
    "    \"\"\"\n",
    "\n",
    "    capacity_multiplier = {\n",
    "        'tiny': 4, 'small': 8, 'medium': 16, 'large': 24, 'full': 32\n",
    "    }[model_capacity]\n",
    "\n",
    "    layers = [1, 2, 3, 4, 5, 6]\n",
    "    filters = [n * capacity_multiplier for n in [32, 4, 4, 4, 8, 16]]\n",
    "    widths = [512, 64, 64, 64, 64, 64]\n",
    "    strides = [(4, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]\n",
    "\n",
    "    x = Input(shape=(1024,), name='input', dtype='float32')\n",
    "    y = Reshape(target_shape=(1024, 1, 1), name='input-reshape')(x)\n",
    "\n",
    "    for l, f, w, s in zip(layers, filters, widths, strides):\n",
    "        y = Conv2D(f, (w, 1), strides=s, padding='same',\n",
    "                   activation='relu', name=\"conv%d\" % l)(y)\n",
    "        y = BatchNormalization(name=\"conv%d-BN\" % l)(y)\n",
    "        y = MaxPool2D(pool_size=(2, 1), strides=None, padding='valid',\n",
    "                      name=\"conv%d-maxpool\" % l)(y)\n",
    "        y = Dropout(0.25, name=\"conv%d-dropout\" % l)(y)\n",
    "\n",
    "    y = Permute((2, 1, 3), name=\"transpose\")(y)\n",
    "    y = Flatten(name=\"flatten\")(y)\n",
    "    y = Dense(360, activation='sigmoid', name=\"classifier\")(y)\n",
    "\n",
    "    model = Model(inputs=x, outputs=y)\n",
    "\n",
    "    model.load_weights(filename)\n",
    "    model.compile('adam', 'binary_crossentropy')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c30084c-eff3-40d0-b860-68a81c9a5b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-22 18:01:15.303050: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-09-22 18:01:15.303082: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-09-22 18:01:15.303110: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (gldsn-hw): /proc/driver/nvidia/version does not exist\n",
      "2021-09-22 18:01:15.303391: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "pitch_model = build_and_load_model(\"medium\", \"../models/model-medium.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72ee5e6-867d-47c7-bceb-a49123cb4ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "919f1917-bf3a-4bbb-904e-e0956cbc79de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29a084f5-52d0-4d9a-aa18-45c1dbb0d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tensorflow.lite.TFLiteConverter.from_keras_model(pitch_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74cd3317-425a-4e29-abce-cb63596674d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-22 18:01:16.930682: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpj54i_2ma/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-22 18:01:19.017304: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2021-09-22 18:01:19.017531: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2021-09-22 18:01:19.038531: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2994400000 Hz\n",
      "2021-09-22 18:01:19.044223: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1144] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.024ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.002ms.\n",
      "\n",
      "2021-09-22 18:01:19.398844: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:345] Ignored output_format.\n",
      "2021-09-22 18:01:19.398892: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:348] Ignored drop_control_dependency.\n",
      "2021-09-22 18:01:19.426382: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f820d39-22ff-497e-ba88-c17aaa8e8cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/crepe-medium.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e7e528b-ee2c-46ae-a40d-3278ef6a6195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tflite_support import metadata as tf_metadata\n",
    "from tflite_support import metadata_schema_py_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "354f887f-ad24-4905-8514-1c6a533af228",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_meta = metadata_schema_py_generated.ModelMetadataT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3b41727-dc37-4526-9acf-6b1bb82e60ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_meta.name = \"Crepe medium\"\n",
    "model_meta.description = \"Pitch detection from wavfile\"\n",
    "model_meta.author = \"https://github.com/marl/\"\n",
    "model_meta.license = (\"MIT License, https://github.com/marl/crepe/blob/master/LICENSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7f258f-319b-40a9-bb21-0d520ede9835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "848e8c7f-4bb1-4f94-a288-8e1241e48a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_meta = metadata_schema_py_generated.TensorMetadataT()\n",
    "\n",
    "output_meta = metadata_schema_py_generated.TensorMetadataT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "387e9b8a-7680-4907-8105-296c00ba88a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_meta.name = \"waveform signal\"\n",
    "input_meta.description = (\n",
    "    \"Input waveform samples array is expected to have shape (n_samples, 1024). \"\n",
    "    \"Every signal should by normalized using its own mean and std. \"\n",
    ")\n",
    "input_meta.content = metadata_schema_py_generated.ContentT()\n",
    "\n",
    "input_meta.content.contentProperties = metadata_schema_py_generated.AudioPropertiesT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6e55188-5769-4454-af77-267c16af2341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_lite_support.metadata.metadata_schema_py_generated.AudioPropertiesT at 0x7fa7186a2d60>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_meta.content.contentProperties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7993dd69-be0a-443e-8763-1c40841c9fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_lite_support.metadata.metadata_schema_py_generated.AudioPropertiesT at 0x7fa7186a2d60>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_meta.content.contentProperties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd92611f-b5a8-4fdc-bec6-b99c153826e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_meta.content.contentProperties.channels = 1\n",
    "input_meta.content.contentProperties.sampleRate = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0736c518-d6e4-4b2a-887f-40556c7d85fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_meta.name = \"Pitch bins probs\"\n",
    "output_meta.description = (\n",
    "    \"Each pitch bin size is 20 cents. abilitOut shape is (n_samples, 360)\"\n",
    "    \"For more info please check out the CREPE docs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b99f7e-b807-4942-bada-8798b3fefca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "42d2238d-feb5-4b0f-b53c-7c185f072eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph = metadata_schema_py_generated.SubGraphMetadataT()\n",
    "subgraph.inputTensorMetadata = [input_meta]\n",
    "subgraph.outputTensorMetadata = [output_meta]\n",
    "model_meta.subgraphMetadata = [subgraph]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aff825a-5cd1-43ad-97de-134a5f21048f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "54add6bd-6579-4932-aae5-88ca1f8caabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tflite_support import flatbuffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca5823e-0475-4b85-97a8-fc9fc4a6a876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64b2a6f7-44df-419b-8899-44b35d4a323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_builder = flatbuffers.Builder(0)\n",
    "meta_builder.Finish(\n",
    "            model_meta.Pack(meta_builder),\n",
    "            tf_metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)\n",
    "metadata_buf = meta_builder.Output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9756b172-aa26-4ba9-a1fb-4ac23feca06d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9fa9adb1-f464-4a2d-93ca-65c92b48214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "populator = tf_metadata.MetadataPopulator.with_model_file(\"../models/crepe-medium.tflite\")\n",
    "populator.load_metadata_buffer(metadata_buf)\n",
    "populator.populate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d866a8af-8ef2-426d-8af7-7fe17af1116d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346216f9-ba68-4017-ba36-cf3f49c9423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../models/crepe-medium.tflite', 'wb') as f:\n",
    "#     f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53069df-acce-4c57-97ca-e63d4995808e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436f09d4-fa0d-4a7a-804b-f797bb70742b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
