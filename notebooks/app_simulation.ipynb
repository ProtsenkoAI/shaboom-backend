{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "451c0b30-6296-462b-9701-2aadf4b83640",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "* pitch detection rip out non-voice\n",
    "\n",
    "### Metric candidates:\n",
    "* absolute error between target and voiced pitch (at every moment)\n",
    "* Raw Pitch Accuracy (for whole song)\n",
    "* proportion f1 for voicing/ silence periods (later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89dd8fd2-1d89-407f-a42a-ab916f704faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "playsound is relying on another python subprocess. Please use `pip install pygobject` if you want playsound to run more efficiently.\n",
      "2021-08-14 01:53:33.594786: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-08-14 01:53:33.594811: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "import IPython.display as ipd\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import demucs\n",
    "import demucs.utils\n",
    "import demucs.separate\n",
    "import demucs.pretrained\n",
    "\n",
    "from resampy import resample\n",
    "import playsound\n",
    "import sounddevice as sd\n",
    "\n",
    "from tensorflow.keras.layers import Input, Reshape, Conv2D, BatchNormalization\n",
    "from tensorflow.keras.layers import MaxPool2D, Dropout, Permute, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7351126-c92c-4f70-92a8-30da30738c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.use(\"Qt5Agg\")\n",
    "\n",
    "MODEL_SR = 16000\n",
    "BLOCK_SIZE = 1024\n",
    "N_CHANNELS = 2\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "PITCH_MODEL_SR = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d80859dd-9ba1-44a0-84e7-fb0c6264c8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocals(mix_pth, model, shifts: int = 1, splits: int=1, overlap: float = 0.25):\n",
    "    loaded_mix = demucs.separate.load_track(mix_pth, DEVICE, N_CHANNELS, model.samplerate)\n",
    "    ref = loaded_mix.mean(0)\n",
    "    normalized_mix = (loaded_mix - ref.mean()) / ref.std()\n",
    "    \n",
    "    all_sources =  demucs.utils.apply_model(model, normalized_mix, shifts=shifts, split=splits,\n",
    "                                    overlap=overlap, progress=True)\n",
    "    return all_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf904210-ed82-4287-8fcc-aa11dd613ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_model = demucs.pretrained.load_pretrained(\"demucs_quantized\")\n",
    "\n",
    "sources_srate = sources_model.samplerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f951e2ea-f6cd-4aca-a497-14e2731bc671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 240.0/240.0 [02:09<00:00,  1.86seconds/s]\n"
     ]
    }
   ],
   "source": [
    "songs = [\n",
    "    \"../data/samples/Arctic Monkeys-MadSounds.mp3\",\n",
    "    \"../data/samples/arctic_monkeys_still_take_you_home.mp3\",\n",
    "    \"../data/samples/arctic-monkeys_mardy-bum.mp3\",\n",
    "    \"../data/samples/radioactive.mp3\",\n",
    "]\n",
    "\n",
    "song_voice_stems = {}\n",
    "\n",
    "for song_pth in songs:\n",
    "    sources = get_vocals(song_pth, sources_model, shifts=1)\n",
    "    vocals_source_idx = sources_model.sources.index(\"vocals\")\n",
    "    song_voice_stems[song_pth] = sources[vocals_source_idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e56ba9bb-66ce-4733-b19a-4adb20f1d345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del sources_model\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e4ffc28-fe24-4a0a-b93f-2926026818b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect_pitch(signal, pitch_model):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    split_idxs = np.arange(1024, len(signal), 1024)\n",
    "    frames = np.split(signal, split_idxs)\n",
    "\n",
    "    last = frames[-1]\n",
    "        \n",
    "    if len(last) < 1024:\n",
    "        need_to_pad = 1024 - len(last)\n",
    "        right_zeros = need_to_pad // 2\n",
    "        left_zeros = need_to_pad - right_zeros\n",
    "        frames[-1] = np.concatenate([np.zeros((left_zeros, 1)), last, np.zeros((right_zeros, 1))])\n",
    "        \n",
    "    frames = np.concatenate(frames, axis=1)\n",
    "    frames = frames.transpose(1, 0) # had shape (1024, n_samples), converted to (n_samples, 1024)\n",
    "\n",
    "    # normalize each frame -- this is expected by the model\n",
    "    frames -= np.mean(frames, axis=1)[:, np.newaxis]\n",
    "    std = np.std(frames, axis=1)[:, np.newaxis]\n",
    "    std[std == 0] = 1e-10\n",
    "    frames /= std\n",
    "    \n",
    "    \n",
    "    model_preds = pitch_model(frames, training=False)#, workers=-1, use_multiprocessing=True)\n",
    "    model_preds = model_preds.numpy()\n",
    "    \n",
    "    # initially has out shape (length, 360), reducing\n",
    "    too_low_too_high_mask = np.array([True] * 80 + [False] * 140 + [True] * 140)\n",
    "    print(too_low_too_high_mask.shape)\n",
    "    model_preds[:, too_low_too_high_mask] = 0\n",
    "    \n",
    "#     print(\"time needed\", time.time() - start_time)\n",
    "    batch_pitch = model_preds.argmax(axis=1)\n",
    "    confidence = model_preds.max(axis=1)\n",
    "    \n",
    "    return batch_pitch, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17fb14fe-bcc2-4ddb-bd29-0a38a74c28bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_and_load_model(model_capacity, filename):\n",
    "    \"\"\"\n",
    "    Build the CNN model and load the weights\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_capacity : 'tiny', 'small', 'medium', 'large', or 'full'\n",
    "        String specifying the model capacity, which determines the model's\n",
    "        capacity multiplier to 4 (tiny), 8 (small), 16 (medium), 24 (large),\n",
    "        or 32 (full). 'full' uses the model size specified in the paper,\n",
    "        and the others use a reduced number of filters in each convolutional\n",
    "        layer, resulting in a smaller model that is faster to evaluate at the\n",
    "        cost of slightly reduced pitch estimation accuracy.\n",
    "    Returns\n",
    "    -------\n",
    "    model : tensorflow.keras.models.Model\n",
    "        The pre-trained keras model loaded in memory\n",
    "    \"\"\"\n",
    "\n",
    "    capacity_multiplier = {\n",
    "        'tiny': 4, 'small': 8, 'medium': 16, 'large': 24, 'full': 32\n",
    "    }[model_capacity]\n",
    "\n",
    "    layers = [1, 2, 3, 4, 5, 6]\n",
    "    filters = [n * capacity_multiplier for n in [32, 4, 4, 4, 8, 16]]\n",
    "    widths = [512, 64, 64, 64, 64, 64]\n",
    "    strides = [(4, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]\n",
    "\n",
    "    x = Input(shape=(1024,), name='input', dtype='float32')\n",
    "    y = Reshape(target_shape=(1024, 1, 1), name='input-reshape')(x)\n",
    "\n",
    "    for l, f, w, s in zip(layers, filters, widths, strides):\n",
    "        y = Conv2D(f, (w, 1), strides=s, padding='same',\n",
    "                   activation='relu', name=\"conv%d\" % l)(y)\n",
    "        y = BatchNormalization(name=\"conv%d-BN\" % l)(y)\n",
    "        y = MaxPool2D(pool_size=(2, 1), strides=None, padding='valid',\n",
    "                      name=\"conv%d-maxpool\" % l)(y)\n",
    "        y = Dropout(0.25, name=\"conv%d-dropout\" % l)(y)\n",
    "\n",
    "    y = Permute((2, 1, 3), name=\"transpose\")(y)\n",
    "    y = Flatten(name=\"flatten\")(y)\n",
    "    y = Dense(360, activation='sigmoid', name=\"classifier\")(y)\n",
    "\n",
    "    model = Model(inputs=x, outputs=y)\n",
    "\n",
    "    model.load_weights(filename)\n",
    "    model.compile('adam', 'binary_crossentropy')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90c29d34-994f-4e65-9a87-4ca7b2e17126",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_realtime_pitch_model = build_and_load_model(\"full\", \"../models/model-full.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e79bac9-8751-4c63-9dae-830c0515be6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/snakers4/silero-vad/archive/master.zip\" to /home/arseny/.cache/torch/hub/master.zip\n"
     ]
    }
   ],
   "source": [
    "vad_model, _ = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
    "                              model='silero_vad',\n",
    "                              force_reload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ac50e60-8ab9-4f25-b860-e21d3e5dbf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_voicing_probs(model, wav, num_samples_per_window: int = 4000, num_steps: int = 8, batch_size=200):\n",
    "    \n",
    "    num_samples = num_samples_per_window\n",
    "    assert num_samples % num_steps == 0\n",
    "    step = int(num_samples / num_steps)  # stride / hop\n",
    "    \n",
    "    outs = []\n",
    "    to_concat = []\n",
    "    for i in range(0, len(wav), step):\n",
    "        chunk = wav[i: i+num_samples]\n",
    "        if len(chunk) < num_samples:\n",
    "            chunk = F.pad(chunk, (0, num_samples - len(chunk)))\n",
    "        to_concat.append(chunk.unsqueeze(0))\n",
    "        if len(to_concat) >= batch_size:\n",
    "            chunks = torch.Tensor(torch.cat(to_concat, dim=0))\n",
    "            with torch.no_grad():\n",
    "                out = model(chunks)\n",
    "            outs.append(out)\n",
    "            to_concat = []\n",
    "\n",
    "    outs = torch.cat(outs, dim=0)\n",
    "    return outs[:, 1] # 1 dim is 'neg' and 'pos' classes, so take pos probability\n",
    "\n",
    "def get_voice_activity_mask(wav, model, sr, thresh=0.02):\n",
    "    \n",
    "    transform = torchaudio.transforms.Resample(orig_freq=sr,\n",
    "                                               new_freq=16000)\n",
    "    src_len = len(wav)\n",
    "    wav = transform(wav)\n",
    "    sr = 16000\n",
    "    \n",
    "    probs = get_voicing_probs(model, wav)\n",
    "    wav_prob = np.full(src_len, 0, dtype=np.float32)\n",
    "\n",
    "    step = src_len / len(probs)\n",
    "    start = 0\n",
    "\n",
    "    for prob in probs.flatten().numpy():\n",
    "        wav_prob[round(start): round(start + step)] = prob\n",
    "        start += step\n",
    "    print(\"quantiles\", np.quantile(wav_prob, 0.01), np.quantile(wav_prob, 0.5), np.quantile(wav_prob, 0.95))\n",
    "    winsize = 16000\n",
    "    rolling_mean_wav_prob = np.convolve(wav_prob, np.ones(winsize), 'same') / winsize\n",
    "    return rolling_mean_wav_prob >= thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9aee3be1-b9e7-4dd0-98f9-163319f69ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantiles 0.006338110659271479 0.2759883403778076 0.9464583396911621\n",
      "(360,)\n",
      "confidence 0.003184974193572998 0.6305088400840759 0.9440664052963257\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "songs_target_pitch = {}\n",
    "\n",
    "for song_pth, vocals in song_voice_stems.items():\n",
    "    mono_vocals = vocals.mean(axis=0, keepdims=False)\n",
    "    activity_mask = get_voice_activity_mask(mono_vocals, vad_model, 44100, thresh=0.04)\n",
    "    mono_vocals = mono_vocals.numpy()\n",
    "    \n",
    "    pure_tone = librosa.tone(frequency=5, sr=44100, length=len(mono_vocals))\n",
    "    \n",
    "    mono_vocals[~activity_mask] = pure_tone[~activity_mask]\n",
    "    \n",
    "    mono_vocals_resampled = resample(mono_vocals, sources_srate, PITCH_MODEL_SR)\n",
    "    \n",
    "    detected_pitch, pitch_confidence = detect_pitch(mono_vocals_resampled.reshape(-1, 1), not_realtime_pitch_model)\n",
    "    print(\"confidence\", np.quantile(pitch_confidence, 0.01), np.quantile(pitch_confidence, 0.5), np.quantile(pitch_confidence, 0.95))\n",
    "    detected_pitch = detected_pitch.astype(np.float32)\n",
    "    detected_pitch[pitch_confidence < 0.65] = None\n",
    "    \n",
    "    songs_target_pitch[song_pth] = detected_pitch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7ff478c8-79aa-4003-b114-379d97a996ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f39e0519a60>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(detected_pitch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "213a6d87-96e5-499f-a624-bf324d190684",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pause(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c37747c-5d9d-4d7a-b0ca-612e9cdfc792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del not_realtime_pitch_model\n",
    "# import gc\n",
    "# gc.collect()\n",
    "\n",
    "pitch_model = build_and_load_model(\"large\", \"../models/model-large.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "098b2d22-43e8-448b-a507-004d9993e2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plotter:\n",
    "    def __init__(self, targ_pitch, n_points = 200, user_plotted_points = 60, mae_npoints = 30):\n",
    "        \n",
    "        assert user_plotted_points <= n_points\n",
    "        self.n_points = n_points\n",
    "        self.mae_npoints = mae_npoints\n",
    "        self.user_plotted_points = user_plotted_points\n",
    "        self.n_target_future_points = n_points - user_plotted_points\n",
    "        \n",
    "        self.user_pitch_padding = np.full(self.n_target_future_points, np.nan)\n",
    "        \n",
    "\n",
    "        points = []\n",
    "        passed_detected_pitch = []\n",
    "\n",
    "        self.fig, axes = plt.subplots(ncols=2, figsize=(18, 6))\n",
    "        self.pitch_ax, self.metrics_ax = axes\n",
    "        \n",
    "        self.pitch_ax.set_ylim((60, 300))\n",
    "        self.metrics_ax.set_ylim((0, 100))\n",
    "        self.metrics_ax.set_xlim((0, self.n_points))\n",
    "        print(self.pitch_ax, self.metrics_ax)\n",
    "\n",
    "        # animated=True tells matplotlib to only draw the artist when we\n",
    "        # explicitly request it\n",
    "        self.user_pitch_arr = np.zeros(self.user_plotted_points)\n",
    "        \n",
    "        self.target_past = np.zeros(self.user_plotted_points).astype(np.float32)\n",
    "        self.target_future = targ_pitch[:self.n_target_future_points].astype(np.float32)\n",
    "#         self.target_pitch_arr = np.concatenate([target_past, target_future])\n",
    "        \n",
    "        self.targ_pitch_queue = list(targ_pitch[self.n_target_future_points:])\n",
    "        \n",
    "        \n",
    "        self.mae_arr = np.full(self.n_points, np.nan)\n",
    "        \n",
    "        self.user_pitch_plot = self._create_plot(np.concatenate([self.user_pitch_arr, self.user_pitch_padding]), self.pitch_ax)\n",
    "        # different colors for already passed target and future target\n",
    "        self.target_past_plot = self._create_plot(self.target_past, self.pitch_ax, c=\"red\")\n",
    "        self.target_future_plot = self._create_plot(self.target_future, self.pitch_ax, c=\"orange\", x=range(len(self.target_past), self.n_points))\n",
    "        \n",
    "        self.mae_plot = self._create_plot(self.mae_arr, self.metrics_ax, c=\"red\")\n",
    "        \n",
    "#         (plotted_data,) = self.pitch_ax.plot(range(n_points), [60] * (n_points - 1) + [300], animated=True)\n",
    "#         (plotted_detected,) = ax.plot(range(n_points), [50] + [310] * (n_points - 1), animated=True, c=\"orange\")\n",
    "        plt.show(block=False)\n",
    "        plt.pause(0.1)\n",
    "\n",
    "        self._bg = self.fig.canvas.copy_from_bbox(self.fig.bbox)\n",
    "        # draw the animated artist, this uses a cached renderer\n",
    "        self.axes = {\n",
    "                    \"pitch\": {\"artists\": [], \"ax\": self.pitch_ax},\n",
    "                    \"metrics\": {\"artists\": [], \"ax\": self.metrics_ax}\n",
    "        }\n",
    "        \n",
    "        self.axes[\"pitch\"][\"artists\"] += [self.user_pitch_plot, self.target_past_plot, self.target_future_plot]\n",
    "        self.axes[\"pitch\"][\"artists\"] += [self.mae_plot]\n",
    "        \n",
    "        self._redraw_plot()\n",
    "        \n",
    "    def _create_plot(self, data, ax, c=\"blue\", x=None):\n",
    "        if x is None:\n",
    "            x = range(len(data))\n",
    "        (plot,) = ax.plot(x, data, animated=True, c=c)\n",
    "        return plot\n",
    "        \n",
    "    def update(self, user_pitch):\n",
    "        self.target_past = np.append(self.target_past[1:], [self.target_future[0]])\n",
    "        \n",
    "        if len(self.targ_pitch_queue): # at the end of song have no target pitch in queue\n",
    "            next_targ_pitch = self.targ_pitch_queue.pop(0)\n",
    "        else:\n",
    "            \n",
    "            next_targ_pitch = np.nan\n",
    "            \n",
    "        self.target_future = np.append(self.target_future[1:], [next_targ_pitch])\n",
    "                                     \n",
    "                                     \n",
    "        self.user_pitch_arr = np.append(self.user_pitch_arr[1:], [user_pitch])\n",
    "        self.mae_arr = np.append(self.mae_arr[1:], [self._calc_next_mae(self.target_past, self.user_pitch_arr)])\n",
    "        \n",
    "#         plotted_data.set_data(indexes[not_nan_mask], new_plot_data[not_nan_mask])\n",
    "        self.user_pitch_plot.set_ydata(np.concatenate([self.user_pitch_arr, self.user_pitch_padding]))\n",
    "        self.target_past_plot.set_ydata(self.target_past)\n",
    "        self.target_future_plot.set_ydata(self.target_future)\n",
    "        self.mae_plot.set_ydata(self.mae_arr)\n",
    "        \n",
    "        self._redraw_plot()\n",
    "        \n",
    "    def _calc_next_mae(self, targ, pred):\n",
    "        mae_arr1, mae_arr2 = targ[-1 * self.mae_npoints:], pred[-1 * self.mae_npoints:]\n",
    "        both_notnull_mask = (~np.isnan(mae_arr1)) & (~np.isnan(mae_arr2))\n",
    "        \n",
    "        mae_arr1 = mae_arr1[both_notnull_mask]\n",
    "        mae_arr2 = mae_arr2[both_notnull_mask]\n",
    "        if len(mae_arr1) == 0: # silence on intersection\n",
    "            value = np.nan\n",
    "        value = np.mean(np.abs(mae_arr1 - mae_arr2))\n",
    "        return value\n",
    "                                       \n",
    "    def _redraw_plot(self):\n",
    "        self.fig.canvas.restore_region(self._bg)\n",
    "        \n",
    "        for ax_name, ax_obj in self.axes.items(): \n",
    "            ax = ax_obj[\"ax\"]\n",
    "            for artist in ax_obj[\"artists\"]:\n",
    "                ax.draw_artist(artist)\n",
    "                             \n",
    "        # show the result to the screen, this pushes the updated RGBA buffer from the\n",
    "        # renderer to the GUI framework so you can see it\n",
    "        self.fig.canvas.blit(self.fig.bbox)\n",
    "        self.fig.canvas.flush_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f22fd21-c65d-4dfb-9857-0332ef2ead79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect_pitch_realtime(signal, model):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    frames = signal[:1024].reshape(1, -1)\n",
    "    frames -= np.mean(frames, axis=1)[:, np.newaxis]\n",
    "    frames /= np.std(frames, axis=1)[:, np.newaxis]\n",
    "    \n",
    "    model_preds = model(frames, training=False)#, workers=-1, use_multiprocessing=True)\n",
    "    model_preds = model_preds.numpy()\n",
    "    \n",
    "    too_low_too_high_mask = np.concatenate([[True] * 80 + [False] * 140 + [True] * 140])\n",
    "    model_preds[:, too_low_too_high_mask] = 0\n",
    "#     print(\"time needed\", time.time() - start_time)\n",
    "    batch_pitch = model_preds.argmax(axis=1)\n",
    "    confidence = model_preds.max(axis=1)\n",
    "    \n",
    "    return batch_pitch, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "13346ee6-5681-455c-891d-b9744950dcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_song(f, mono=False):\n",
    "    \"\"\"MP3 to numpy array\"\"\"\n",
    "    a = AudioSegment.from_mp3(f)\n",
    "    y = np.array(a.get_array_of_samples())\n",
    "    if a.channels == 2:\n",
    "        y = y.reshape((-1, 2))\n",
    "        if mono:\n",
    "            y = y.mean(axis=1)\n",
    "    return y, a.frame_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1e2ba88e-2b19-4aed-b0ec-282d7754188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import pygame\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "    \n",
    "\n",
    "class SongPlayer:\n",
    "    def __init__(self, song_pth, bounds):\n",
    "        print(\"bounds\", bounds)\n",
    "        target_pitch = songs_target_pitch[song_pth]\n",
    "        \n",
    "        tg_len = len(target_pitch)\n",
    "        start, end = bounds\n",
    "        # initially in range 0-100\n",
    "        start /= 100\n",
    "        end /= 100\n",
    "        \n",
    "        cropped_targ_pitch = target_pitch[int(start * tg_len): int(end * tg_len)]\n",
    "        \n",
    "        song_mono, srate = load_song(song_pth, mono=True)\n",
    "        print(\"src shape\", song_mono.shape, srate)\n",
    "        song_ln = len(song_mono)\n",
    "        cropped_song = song_mono[int(start * song_ln): int(end * song_ln)]\n",
    "        print(\"cropped\", cropped_song)\n",
    "        \n",
    "        self.song_pth = \"now_played_song.wav\"\n",
    "        write(self.song_pth, srate, cropped_song.astype(np.int16))\n",
    "        \n",
    "        AudioSegment.from_wav(self.song_pth).export('now_played_song.ogg', format='ogg')\n",
    "        \n",
    "        pygame.mixer.init()\n",
    "        pygame.mixer.music.load(\"now_played_song.ogg\")\n",
    "        \n",
    "        self.targ_pitch = cropped_targ_pitch\n",
    "        \n",
    "        self.stream = None\n",
    "        self.plotter = None\n",
    "        \n",
    "    def start(self):\n",
    "        self.stream = sd.InputStream(\n",
    "                        samplerate=MODEL_SR,\n",
    "                        blocksize = BLOCK_SIZE,\n",
    "                        channels = 1,\n",
    "        )\n",
    "        \n",
    "        self.plotter = Plotter(self.targ_pitch)\n",
    "        \n",
    "        self.stream.start()\n",
    "        pygame.mixer.music.play()\n",
    "        \n",
    "    def play(self):\n",
    "        for block_idx in range(len(self.targ_pitch)):\n",
    "            user_pitch = self._get_user_pitch()\n",
    "            self.plotter.update(user_pitch)\n",
    "            yield\n",
    "            \n",
    "        self.stop()\n",
    "        \n",
    "    def _get_user_pitch(self):  \n",
    "        audio_arr, is_overflowed = self.stream.read(BLOCK_SIZE)\n",
    "        if is_overflowed:\n",
    "            raise OverflowError()\n",
    "\n",
    "        model_preds, confidence = detect_pitch_realtime(audio_arr, pitch_model)\n",
    "        model_preds = model_preds.astype(np.float32)\n",
    "        model_preds[confidence < 0.5] = None\n",
    "        \n",
    "        assert len(model_preds) == 1\n",
    "        return model_preds[0]\n",
    "        \n",
    "    def stop(self):\n",
    "        pygame.mixer.music.stop()\n",
    "        self.stream.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3dac02-bd13-49ef-bfff-62ae634bba78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b53c33cf-58a9-4386-bc43-706899032a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fragment_bounds():\n",
    "    start = input(\"start percentile\\n\")\n",
    "    if len(start) == 0:\n",
    "        start = 0\n",
    "    else:\n",
    "        start = int(start)\n",
    "    end = input(\"end percentile\\n\")\n",
    "    if len(end) == 0:\n",
    "        end = 100\n",
    "    else:\n",
    "        end = int(end)\n",
    "        \n",
    "    return start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "00cc1696-6b8d-4d2f-9b0e-5b6c4c5f2eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_playing():\n",
    "    print(\"If move to next track, pass N\")\n",
    "    print(\"If repeat this track, pass R\")\n",
    "    print(\"If previous track, pass P\")\n",
    "    value = input()\n",
    "    return {\"N\": \"next\", \"R\": \"repeat\", \"P\": \"previous\"}[value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f4a86a-66c5-4bcb-a20a-dbfb91421c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9f4c982d-3eec-4fcd-bafc-05019f0d7338",
   "metadata": {},
   "outputs": [],
   "source": [
    "click_sound_pth = \"../data/samples/click.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0d972cec-af30-4dc5-96a1-ca137b91cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import librosa\n",
    "# click = librosa.clicks(times=[0], click_duration=1, sr=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "96960b2d-9dce-4ac4-b94b-dadd91a4f408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipd.Audio(click, rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "45024a98-0a01-4426-ac9b-3d78b4863079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy\n",
    "# scipy.io.wavfile.write(click_sound_pth, 44100, click)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ef4dd615-bde8-46bd-9fa0-9331f56eda9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_countdown(n: int):\n",
    "    # TODO: add click sound\n",
    "    for num in range(n, 0, -1):\n",
    "        print(f\"Prepare: {num}\")\n",
    "        playsound.playsound(click_sound_pth, False)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67104f78-ab04-4ee1-a542-2f4ee975a42e",
   "metadata": {},
   "source": [
    "## TODO: STOP BUTTON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8710bc28-0309-42d0-aedb-bcb7152827ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d7709d210b498b9a2a8922a12a21df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Stop song')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "start percentile\n",
      " \n",
      "end percentile\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bounds (0, 100)\n",
      "src shape (9484325,) 44100\n",
      "cropped [0. 0. 0. ... 0. 0. 0.]\n",
      "Prepare: 3\n",
      "Prepare: 2\n",
      "Prepare: 1\n",
      "AxesSubplot(0.125,0.125;0.352273x0.755) AxesSubplot(0.547727,0.125;0.352273x0.755)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arseny/.local/share/virtualenvs/music-ai-trainer-TlEmgf2U/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/arseny/.local/share/virtualenvs/music-ai-trainer-TlEmgf2U/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_44193/3085061236.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmusic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"all\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_44193/3085061236.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcheck_need_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_44193/4280258775.py\u001b[0m in \u001b[0;36mplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarg_pitch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0muser_pitch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_user_pitch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_pitch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_44193/4280258775.py\u001b[0m in \u001b[0;36m_get_user_pitch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mOverflowError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mmodel_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_pitch_realtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpitch_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mmodel_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mmodel_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfidence\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_44193/4065211434.py\u001b[0m in \u001b[0;36mdetect_pitch_realtime\u001b[0;34m(signal, model)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mframes\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, workers=-1, use_multiprocessing=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mmodel_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/music-ai-trainer-TlEmgf2U/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1030\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/music-ai-trainer-TlEmgf2U/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mone\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m     \"\"\"\n\u001b[0;32m--> 420\u001b[0;31m     return self._run_internal_graph(\n\u001b[0m\u001b[1;32m    421\u001b[0m         inputs, training=training, mask=mask)\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/music-ai-trainer-TlEmgf2U/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/music-ai-trainer-TlEmgf2U/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1030\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/music-ai-trainer-TlEmgf2U/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m               outputs, _apply_fn, inner_rank=self.rank + 1)\n\u001b[1;32m    266\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m           outputs = nn.bias_add(\n\u001b[0m\u001b[1;32m    268\u001b[0m               outputs, self.bias, data_format=self._tf_data_format)\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/music-ai-trainer-TlEmgf2U/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/music-ai-trainer-TlEmgf2U/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m   3375\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3376\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3377\u001b[0;31m       return gen_nn_ops.bias_add(\n\u001b[0m\u001b[1;32m   3378\u001b[0m           value, bias, data_format=data_format, name=name)\n\u001b[1;32m   3379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/music-ai-trainer-TlEmgf2U/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m    672\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m    675\u001b[0m         _ctx, \"BiasAdd\", name, value, bias, \"data_format\", data_format)\n\u001b[1;32m    676\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "song_idx = 0\n",
    "\n",
    "next_play_name_to_idx_change = {\n",
    "    \"repeat\": 0,\n",
    "    \"next\": 1,\n",
    "    \"previous\": -1,\n",
    "}\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ipd.display(need_stop_widg)\n",
    "        # input to choose part of song to play\n",
    "        fragment_bounds = get_fragment_bounds()\n",
    "\n",
    "        player = SongPlayer(songs[song_idx], fragment_bounds)\n",
    "\n",
    "        do_countdown(3)\n",
    "        player.start()\n",
    "\n",
    "        for step in player.play():\n",
    "            ...\n",
    "            \n",
    "        next_playing = get_next_playing()\n",
    "        song_idx += next_play_name_to_idx_change[next_playing]\n",
    "        \n",
    "except BaseException as e:\n",
    "    pygame.mixer.music.stop()\n",
    "    plt.close(\"all\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd150f2-245c-4ad6-8647-f711a9efc9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2d66cd-8b69-451a-b245-62010bdee4da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dec1ca-c3c4-4159-b97d-b97137a0cf02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
