{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "451c0b30-6296-462b-9701-2aadf4b83640",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "* pitch detection rip out non-voice\n",
    "\n",
    "### Metric candidates:\n",
    "* absolute error between target and voiced pitch (at every moment)\n",
    "* Raw Pitch Accuracy (for whole song)\n",
    "* proportion f1 for voicing/ silence periods (later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89dd8fd2-1d89-407f-a42a-ab916f704faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "playsound is relying on another python subprocess. Please use `pip install pygobject` if you want playsound to run more efficiently.\n",
      "2021-08-13 15:05:48.361941: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-08-13 15:05:48.361983: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "import IPython.display as ipd\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import demucs\n",
    "import demucs.utils\n",
    "import demucs.separate\n",
    "import demucs.pretrained\n",
    "\n",
    "from resampy import resample\n",
    "import playsound\n",
    "import sounddevice as sd\n",
    "\n",
    "from tensorflow.keras.layers import Input, Reshape, Conv2D, BatchNormalization\n",
    "from tensorflow.keras.layers import MaxPool2D, Dropout, Permute, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7351126-c92c-4f70-92a8-30da30738c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.use(\"Qt5Agg\")\n",
    "\n",
    "MODEL_SR = 16000\n",
    "BLOCK_SIZE = 1024\n",
    "N_CHANNELS = 2\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "PITCH_MODEL_SR = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d80859dd-9ba1-44a0-84e7-fb0c6264c8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocals(mix_pth, model, shifts: int = 1, splits: int=1, overlap: float = 0.25):\n",
    "    loaded_mix = demucs.separate.load_track(mix_pth, DEVICE, N_CHANNELS, model.samplerate)\n",
    "    ref = loaded_mix.mean(0)\n",
    "    normalized_mix = (loaded_mix - ref.mean()) / ref.std()\n",
    "    \n",
    "    all_sources =  demucs.utils.apply_model(model, normalized_mix, shifts=shifts, split=splits,\n",
    "                                    overlap=overlap, progress=True)\n",
    "    return all_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf904210-ed82-4287-8fcc-aa11dd613ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_model = demucs.pretrained.load_pretrained(\"demucs_quantized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "f951e2ea-f6cd-4aca-a497-14e2731bc671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 240.0/240.0 [01:59<00:00,  2.01seconds/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 180.0/180.0 [01:34<00:00,  1.90seconds/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 180.0/180.0 [01:31<00:00,  1.97seconds/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 210.0/210.0 [01:39<00:00,  2.11seconds/s]\n"
     ]
    }
   ],
   "source": [
    "songs = [\n",
    "    \"../data/samples/Arctic Monkeys-MadSounds.mp3\",\n",
    "    \"../data/samples/arctic_monkeys_still_take_you_home.mp3\",\n",
    "    \"../data/samples/arctic-monkeys_mardy-bum.mp3\",\n",
    "    \"../data/samples/radioactive.mp3\",\n",
    "]\n",
    "\n",
    "song_voice_stems = {}\n",
    "\n",
    "for song_pth in songs:\n",
    "    sources = get_vocals(song_pth, sources_model, shifts=1)\n",
    "    vocals_source_idx = sources_model.sources.index(\"vocals\")\n",
    "    song_voice_stems[song_pth] = sources[vocals_source_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "4e4ffc28-fe24-4a0a-b93f-2926026818b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect_pitch(signal, pitch_model):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    split_idxs = np.arange(1024, len(signal), 1024)\n",
    "    frames = np.split(signal, split_idxs)\n",
    "\n",
    "    last = frames[-1]\n",
    "        \n",
    "    if len(last) < 1024:\n",
    "        need_to_pad = 1024 - len(last)\n",
    "        right_zeros = need_to_pad // 2\n",
    "        left_zeros = need_to_pad - right_zeros\n",
    "        frames[-1] = np.concatenate([np.zeros((left_zeros, 1)), last, np.zeros((right_zeros, 1))])\n",
    "        \n",
    "    frames = np.concatenate(frames, axis=1)\n",
    "    frames = frames.transpose(1, 0) # had shape (1024, n_samples), converted to (n_samples, 1024)\n",
    "\n",
    "    # normalize each frame -- this is expected by the model\n",
    "    frames -= np.mean(frames, axis=1)[:, np.newaxis]\n",
    "    frames /= np.std(frames, axis=1)[:, np.newaxis]\n",
    "    \n",
    "    \n",
    "    model_preds = pitch_model(frames, training=False)#, workers=-1, use_multiprocessing=True)\n",
    "    model_preds = model_preds.numpy()\n",
    "    \n",
    "    # initially has out shape (length, 360), reducing\n",
    "    too_low_too_high_mask = np.array([True] * 80 + [False] * 140 + [True] * 140)\n",
    "    print(too_low_too_high_mask.shape)\n",
    "    model_preds[:, too_low_too_high_mask] = 0\n",
    "    \n",
    "#     print(\"time needed\", time.time() - start_time)\n",
    "    batch_pitch = model_preds.argmax(axis=1)\n",
    "    confidence = model_preds.max(axis=1)\n",
    "    \n",
    "    return batch_pitch, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "17fb14fe-bcc2-4ddb-bd29-0a38a74c28bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_and_load_model(model_capacity, filename):\n",
    "    \"\"\"\n",
    "    Build the CNN model and load the weights\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_capacity : 'tiny', 'small', 'medium', 'large', or 'full'\n",
    "        String specifying the model capacity, which determines the model's\n",
    "        capacity multiplier to 4 (tiny), 8 (small), 16 (medium), 24 (large),\n",
    "        or 32 (full). 'full' uses the model size specified in the paper,\n",
    "        and the others use a reduced number of filters in each convolutional\n",
    "        layer, resulting in a smaller model that is faster to evaluate at the\n",
    "        cost of slightly reduced pitch estimation accuracy.\n",
    "    Returns\n",
    "    -------\n",
    "    model : tensorflow.keras.models.Model\n",
    "        The pre-trained keras model loaded in memory\n",
    "    \"\"\"\n",
    "\n",
    "    capacity_multiplier = {\n",
    "        'tiny': 4, 'small': 8, 'medium': 16, 'large': 24, 'full': 32\n",
    "    }[model_capacity]\n",
    "\n",
    "    layers = [1, 2, 3, 4, 5, 6]\n",
    "    filters = [n * capacity_multiplier for n in [32, 4, 4, 4, 8, 16]]\n",
    "    widths = [512, 64, 64, 64, 64, 64]\n",
    "    strides = [(4, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]\n",
    "\n",
    "    x = Input(shape=(1024,), name='input', dtype='float32')\n",
    "    y = Reshape(target_shape=(1024, 1, 1), name='input-reshape')(x)\n",
    "\n",
    "    for l, f, w, s in zip(layers, filters, widths, strides):\n",
    "        y = Conv2D(f, (w, 1), strides=s, padding='same',\n",
    "                   activation='relu', name=\"conv%d\" % l)(y)\n",
    "        y = BatchNormalization(name=\"conv%d-BN\" % l)(y)\n",
    "        y = MaxPool2D(pool_size=(2, 1), strides=None, padding='valid',\n",
    "                      name=\"conv%d-maxpool\" % l)(y)\n",
    "        y = Dropout(0.25, name=\"conv%d-dropout\" % l)(y)\n",
    "\n",
    "    y = Permute((2, 1, 3), name=\"transpose\")(y)\n",
    "    y = Flatten(name=\"flatten\")(y)\n",
    "    y = Dense(360, activation='sigmoid', name=\"classifier\")(y)\n",
    "\n",
    "    model = Model(inputs=x, outputs=y)\n",
    "\n",
    "    model.load_weights(filename)\n",
    "    model.compile('adam', 'binary_crossentropy')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "90c29d34-994f-4e65-9a87-4ca7b2e17126",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_realtime_pitch_model = build_and_load_model(\"full\", \"../models/model-full.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "9aee3be1-b9e7-4dd0-98f9-163319f69ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360,)\n",
      "(360,)\n",
      "(360,)\n",
      "(360,)\n"
     ]
    }
   ],
   "source": [
    "songs_target_pitch = {}\n",
    "\n",
    "for song_pth, vocals in song_voice_stems.items():\n",
    "    mono_vocals = vocals.mean(axis=0, keepdims=True)\n",
    "    mono_vocals = mono_vocals.transpose(1, 0)\n",
    "    mono_vocals_resampled = resample(mono_vocals.flatten().numpy(), sources_model.samplerate, PITCH_MODEL_SR)\n",
    "    \n",
    "    detected_pitch, pitch_confidence = detect_pitch(mono_vocals_resampled.reshape(-1, 1), not_realtime_pitch_model)\n",
    "    \n",
    "    detected_pitch = detected_pitch.astype(np.float32)\n",
    "    detected_pitch[pitch_confidence < 0.65] = None\n",
    "    \n",
    "    songs_target_pitch[song_pth] = detected_pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "7c37747c-5d9d-4d7a-b0ca-612e9cdfc792",
   "metadata": {},
   "outputs": [],
   "source": [
    "del not_realtime_pitch_model\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "pitch_model = build_and_load_model(\"large\", \"../models/model-large.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "098b2d22-43e8-448b-a507-004d9993e2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plotter:\n",
    "    def __init__(self, targ_pitch, n_points = 200, user_plotted_points = 60, mae_npoints = 30):\n",
    "        assert user_plotted_points <= n_points\n",
    "        self.n_points = n_points\n",
    "        self.mae_npoints = mae_npoints\n",
    "        self.user_plotted_points = user_plotted_points\n",
    "        self.n_target_future_points = n_points - user_plotted_points\n",
    "        \n",
    "        self.user_pitch_padding = np.full(self.n_target_future_points, np.nan)\n",
    "        \n",
    "\n",
    "        points = []\n",
    "        passed_detected_pitch = []\n",
    "\n",
    "        self.fig, axes = plt.subplots(ncols=2, figsize=(18, 6))\n",
    "        self.pitch_ax, self.metrics_ax = axes\n",
    "        \n",
    "        self.pitch_ax.set_ylim((60, 300))\n",
    "        self.metrics_ax.set_ylim((0, 100))\n",
    "        self.metrics_ax.set_xlim((0, self.n_points))\n",
    "        print(self.pitch_ax, self.metrics_ax)\n",
    "\n",
    "        # animated=True tells matplotlib to only draw the artist when we\n",
    "        # explicitly request it\n",
    "        self.user_pitch_arr = np.zeros(self.user_plotted_points)\n",
    "        \n",
    "        self.target_past = np.zeros(self.user_plotted_points)\n",
    "        self.target_future = targ_pitch[:self.n_target_future_points]\n",
    "#         self.target_pitch_arr = np.concatenate([target_past, target_future])\n",
    "        \n",
    "        self.targ_pitch_queue = list(targ_pitch[self.n_target_future_points:])\n",
    "        \n",
    "        \n",
    "        self.mae_arr = np.full(self.n_points, np.nan)\n",
    "        \n",
    "        self.user_pitch_plot = self._create_plot(np.concatenate([self.user_pitch_arr, self.user_pitch_padding]), self.pitch_ax)\n",
    "        # different colors for already passed target and future target\n",
    "        self.target_past_plot = self._create_plot(self.target_past, self.pitch_ax, c=\"red\")\n",
    "        self.target_future_plot = self._create_plot(self.target_future, self.pitch_ax, c=\"orange\", x=range(len(self.target_past), self.n_points))\n",
    "        \n",
    "        self.mae_plot = self._create_plot(self.mae_arr, self.metrics_ax, c=\"red\")\n",
    "        \n",
    "#         (plotted_data,) = self.pitch_ax.plot(range(n_points), [60] * (n_points - 1) + [300], animated=True)\n",
    "#         (plotted_detected,) = ax.plot(range(n_points), [50] + [310] * (n_points - 1), animated=True, c=\"orange\")\n",
    "        plt.show(block=False)\n",
    "        plt.pause(0.1)\n",
    "\n",
    "        self._bg = self.fig.canvas.copy_from_bbox(self.fig.bbox)\n",
    "        # draw the animated artist, this uses a cached renderer\n",
    "        self.axes = {\n",
    "                    \"pitch\": {\"artists\": [], \"ax\": self.pitch_ax},\n",
    "                    \"metrics\": {\"artists\": [], \"ax\": self.metrics_ax}\n",
    "        }\n",
    "        \n",
    "        self.axes[\"pitch\"][\"artists\"] += [self.user_pitch_plot, self.target_past_plot, self.target_future_plot]\n",
    "        self.axes[\"pitch\"][\"artists\"] += [self.mae_plot]\n",
    "        \n",
    "        self._redraw_plot()\n",
    "        \n",
    "    def _create_plot(self, data, ax, c=\"blue\", x=None):\n",
    "        if x is None:\n",
    "            x = range(len(data))\n",
    "        (plot,) = ax.plot(x, data, animated=True, c=c)\n",
    "        return plot\n",
    "        \n",
    "    def update(self, user_pitch):\n",
    "        self.target_past = np.append(self.target_past[1:], [self.target_future[0]])\n",
    "        if len(self.targ_pitch_queue): # at the end of song have no target pitch in queue\n",
    "            next_targ_pitch = np.nan\n",
    "        else:\n",
    "            next_targ_pitch = self.targ_pitch_queue.pop(0)\n",
    "        self.target_future = np.append(self.target_future[1:], [next_targ_pitch])\n",
    "                                     \n",
    "                                     \n",
    "        self.user_pitch_arr = np.append(self.user_pitch_arr[1:], [user_pitch])\n",
    "        self.mae_arr = np.append(self.mae_arr[1:], [self._calc_next_mae(self.target_past, self.user_pitch_arr)])\n",
    "        \n",
    "#         plotted_data.set_data(indexes[not_nan_mask], new_plot_data[not_nan_mask])\n",
    "        self.user_pitch_plot.set_ydata(np.concatenate([self.user_pitch_arr, self.user_pitch_padding]))\n",
    "        self.target_past_plot.set_ydata(self.target_past)\n",
    "        self.target_future_plot.set_ydata(self.target_future)\n",
    "        self.mae_plot.set_ydata(self.mae_arr)\n",
    "        \n",
    "        self._redraw_plot()\n",
    "        \n",
    "    def _calc_next_mae(self, targ, pred):\n",
    "        mae_arr1, mae_arr2 = targ[-1 * self.mae_npoints:], pred[-1 * self.mae_npoints:]\n",
    "        both_notnull_mask = (~np.isnan(mae_arr1)) & (~np.isnan(mae_arr2))\n",
    "        \n",
    "        mae_arr1 = mae_arr1[both_notnull_mask]\n",
    "        mae_arr2 = mae_arr2[both_notnull_mask]\n",
    "        if len(mae_arr1) == 0: # silence on intersection\n",
    "            value = np.nan\n",
    "        value = np.mean(np.abs(mae_arr1 - mae_arr2))\n",
    "        return value\n",
    "                                       \n",
    "    def _redraw_plot(self):\n",
    "        self.fig.canvas.restore_region(self._bg)\n",
    "        \n",
    "        for ax_name, ax_obj in self.axes.items(): \n",
    "            ax = ax_obj[\"ax\"]\n",
    "            for artist in ax_obj[\"artists\"]:\n",
    "                ax.draw_artist(artist)\n",
    "                             \n",
    "        # show the result to the screen, this pushes the updated RGBA buffer from the\n",
    "        # renderer to the GUI framework so you can see it\n",
    "        self.fig.canvas.blit(self.fig.bbox)\n",
    "        self.fig.canvas.flush_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "4f22fd21-c65d-4dfb-9857-0332ef2ead79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect_pitch_realtime(signal, model):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    frames = signal[:1024].reshape(1, -1)\n",
    "    frames -= np.mean(frames, axis=1)[:, np.newaxis]\n",
    "    frames /= np.std(frames, axis=1)[:, np.newaxis]\n",
    "    \n",
    "    model_preds = model(frames, training=False)#, workers=-1, use_multiprocessing=True)\n",
    "    model_preds = model_preds.numpy()\n",
    "    \n",
    "    too_low_too_high_mask = np.concatenate([[True] * 80 + [False] * 140 + [True] * 140])\n",
    "    model_preds[:, too_low_too_high_mask] = 0\n",
    "#     print(\"time needed\", time.time() - start_time)\n",
    "    batch_pitch = model_preds.argmax(axis=1)\n",
    "    confidence = model_preds.max(axis=1)\n",
    "    \n",
    "    return batch_pitch, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "13346ee6-5681-455c-891d-b9744950dcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_song(f, mono=False):\n",
    "    \"\"\"MP3 to numpy array\"\"\"\n",
    "    a = AudioSegment.from_mp3(f)\n",
    "    y = np.array(a.get_array_of_samples())\n",
    "    if a.channels == 2:\n",
    "        y = y.reshape((-1, 2))\n",
    "        if mono:\n",
    "            y = y.mean(axis=1)\n",
    "    return y, a.frame_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "1e2ba88e-2b19-4aed-b0ec-282d7754188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import pygame\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "    \n",
    "\n",
    "class SongPlayer:\n",
    "    def __init__(self, song_pth, bounds):\n",
    "        print(\"bounds\", bounds)\n",
    "        target_pitch = songs_target_pitch[song_pth]\n",
    "        \n",
    "        tg_len = len(target_pitch)\n",
    "        start, end = bounds\n",
    "        cropped_targ_pitch = target_pitch[int(start * tg_len): int(end * tg_len)]\n",
    "        \n",
    "        song_mono, srate = load_song(song_pth, mono=True)\n",
    "        print(\"src shape\", song_mono.shape, srate)\n",
    "        song_ln = len(song_mono)\n",
    "        cropped_song = song_mono[int(start * song_ln): int(end * song_ln)]\n",
    "        print(\"cropped\", cropped_song)\n",
    "        \n",
    "        self.song_pth = \"now_played_song.wav\"\n",
    "        write(self.song_pth, srate, cropped_song.astype(np.int16))\n",
    "        \n",
    "        AudioSegment.from_wav(self.song_pth).export('now_played_song.ogg', format='ogg')\n",
    "        \n",
    "        pygame.mixer.init()\n",
    "        pygame.mixer.music.load(\"now_played_song.ogg\")\n",
    "        \n",
    "        self.targ_pitch = cropped_targ_pitch\n",
    "        \n",
    "        self.stream = None\n",
    "        self.plotter = None\n",
    "        \n",
    "    def start(self):\n",
    "        self.stream = sd.InputStream(\n",
    "                        samplerate=MODEL_SR,\n",
    "                        blocksize = BLOCK_SIZE,\n",
    "                        channels = 1,\n",
    "        )\n",
    "        \n",
    "        self.plotter = Plotter(self.targ_pitch)\n",
    "        \n",
    "        \n",
    "        self.stream.start()\n",
    "        pygame.mixer.music.play()\n",
    "        \n",
    "    def play(self):\n",
    "        for block_idx in range(len(self.targ_pitch)):\n",
    "            user_pitch = self._get_user_pitch()\n",
    "            self.plotter.update(user_pitch)\n",
    "            yield\n",
    "            \n",
    "        self.stop()\n",
    "        \n",
    "    def _get_user_pitch(self):  \n",
    "        audio_arr, is_overflowed = self.stream.read(BLOCK_SIZE)\n",
    "        if is_overflowed:\n",
    "            raise OverflowError()\n",
    "\n",
    "        model_preds, confidence = detect_pitch_realtime(audio_arr, pitch_model)\n",
    "        model_preds = model_preds.astype(np.float32)\n",
    "        model_preds[confidence < 0.5] = None\n",
    "        \n",
    "        assert len(model_preds) == 1\n",
    "        return model_preds[0]\n",
    "        \n",
    "    def stop(self):\n",
    "        pygame.mixer.music.stop()\n",
    "        self.stream.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3dac02-bd13-49ef-bfff-62ae634bba78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "b53c33cf-58a9-4386-bc43-706899032a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fragment_bounds():\n",
    "    start = input(\"start percentile\\n\")\n",
    "    if len(start) == 0:\n",
    "        start = 0\n",
    "    else:\n",
    "        start = int(start)\n",
    "    end = input(\"end percentile\\n\")\n",
    "    if len(end) == 0:\n",
    "        end = 100\n",
    "    else:\n",
    "        end = int(end)\n",
    "        \n",
    "    return start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "00cc1696-6b8d-4d2f-9b0e-5b6c4c5f2eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_playing():\n",
    "    print(\"If move to next track, pass N\")\n",
    "    print(\"If repeat this track, pass R\")\n",
    "    print(\"If previous track, pass P\")\n",
    "    value = input()\n",
    "    return {\"N\": \"next\", \"R\": \"repeat\", \"P\": \"previous\"}[value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "e0b6aa89-6329-43dd-a2ae-80b23d10be4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "need_stop_button = widgets.Button(\n",
    "    description='Stop song',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click me',\n",
    "    icon='check' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "button_stop_clicked = False\n",
    "\n",
    "def check_need_stop():\n",
    "    return button_stop_clicked\n",
    "\n",
    "def _button_clicked():\n",
    "    global button_stop_clicked\n",
    "    button_stop_clicked = True\n",
    "    \n",
    "need_stop_button.on_click(_button_clicked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "9f4c982d-3eec-4fcd-bafc-05019f0d7338",
   "metadata": {},
   "outputs": [],
   "source": [
    "click_sound_pth = \"../data/samples/click.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "0d972cec-af30-4dc5-96a1-ca137b91cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import librosa\n",
    "# click = librosa.clicks(times=[0], click_duration=1, sr=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "96960b2d-9dce-4ac4-b94b-dadd91a4f408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipd.Audio(click, rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "45024a98-0a01-4426-ac9b-3d78b4863079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy\n",
    "# scipy.io.wavfile.write(click_sound_pth, 44100, click)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "ef4dd615-bde8-46bd-9fa0-9331f56eda9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_countdown(n: int):\n",
    "    # TODO: add click sound\n",
    "    for num in range(n, 0, -1):\n",
    "        print(f\"Prepare: {num}\")\n",
    "        playsound.playsound(click_sound_pth, False)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67104f78-ab04-4ee1-a542-2f4ee975a42e",
   "metadata": {},
   "source": [
    "## TODO: STOP BUTTON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "8710bc28-0309-42d0-aedb-bcb7152827ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "start percentile\n",
      " \n",
      "end percentile\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bounds (0, 100)\n",
      "src shape (9484325,) 44100\n",
      "cropped [0. 0. 0. ... 0. 0. 0.]\n",
      "Prepare: 3\n",
      "Prepare: 2\n",
      "Prepare: 1\n",
      "AxesSubplot(0.125,0.125;0.352273x0.755) AxesSubplot(0.547727,0.125;0.352273x0.755)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arseny/.local/share/virtualenvs/music-ai-trainer-TlEmgf2U/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/arseny/.local/share/virtualenvs/music-ai-trainer-TlEmgf2U/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "pop from empty list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9103/2561802931.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmusic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"all\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_9103/2561802931.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcheck_need_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9103/3690726003.py\u001b[0m in \u001b[0;36mplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarg_pitch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0muser_pitch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_user_pitch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_pitch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9103/4117092725.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, user_pitch)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_pitch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_past\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_past\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_future\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_future\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_future\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarg_pitch_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from empty list"
     ]
    }
   ],
   "source": [
    "song_idx = 0\n",
    "\n",
    "next_play_name_to_idx_change = {\n",
    "    \"repeat\": 0,\n",
    "    \"next\": 1,\n",
    "    \"previous\": -1,\n",
    "}\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # input to choose part of song to play\n",
    "        fragment_bounds = get_fragment_bounds()\n",
    "\n",
    "        player = SongPlayer(songs[song_idx], fragment_bounds)\n",
    "\n",
    "        do_countdown(3)\n",
    "        player.start()\n",
    "\n",
    "        for step in player.play():\n",
    "            if check_need_stop():\n",
    "                player.stop()\n",
    "                break\n",
    "\n",
    "        next_playing = get_next_playing()\n",
    "        song_idx += next_play_name_to_idx_change[next_playing]\n",
    "except BaseException as e:\n",
    "    pygame.mixer.music.stop()\n",
    "    plt.close(\"all\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd150f2-245c-4ad6-8647-f711a9efc9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433154a5-d1c5-4787-8d23-16f78df27768",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
